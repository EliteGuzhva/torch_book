{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TaxsuHlzxh4g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWKyvqCT05Pt"
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "    def get_data(self, path, batch_size=20):\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words: \n",
        "                    self.dictionary.add_word(word)  \n",
        "        \n",
        "        # Tokenize the file content\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        return ids.view(batch_size, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UDCBwxud05Tk"
      },
      "outputs": [],
      "source": [
        "# Some part of the code was referenced from below.\n",
        "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XDSXTqkQ05WM"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('mps')\n",
        "\n",
        "# Hyper-parameters\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "num_epochs = 5\n",
        "num_samples = 1000     # number of words to be sampled\n",
        "batch_size = 20\n",
        "seq_length = 30\n",
        "learning_rate = 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XfTKIUP905Y_"
      },
      "outputs": [],
      "source": [
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('wikitext2.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Pax5TfmM05cB"
      },
      "outputs": [],
      "source": [
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sQfxq8vk1z5T"
      },
      "outputs": [],
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "u1e-oRmJ1z8D"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utGnS47I1z-o",
        "outputId": "caa6ce1d-105f-452a-ecb0-075add04d883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[0/3481], Loss: 3.6152, Perplexity: 37.16\n",
            "Epoch [1/5], Step[100/3481], Loss: 3.3063, Perplexity: 27.28\n",
            "Epoch [1/5], Step[200/3481], Loss: 3.0094, Perplexity: 20.27\n",
            "Epoch [1/5], Step[300/3481], Loss: 2.7792, Perplexity: 16.11\n",
            "Epoch [1/5], Step[400/3481], Loss: 2.6396, Perplexity: 14.01\n",
            "Epoch [1/5], Step[500/3481], Loss: 2.5636, Perplexity: 12.98\n",
            "Epoch [1/5], Step[600/3481], Loss: 4.5683, Perplexity: 96.38\n",
            "Epoch [1/5], Step[700/3481], Loss: 4.5888, Perplexity: 98.38\n",
            "Epoch [1/5], Step[800/3481], Loss: 5.4054, Perplexity: 222.60\n",
            "Epoch [1/5], Step[900/3481], Loss: 5.6789, Perplexity: 292.63\n",
            "Epoch [1/5], Step[1000/3481], Loss: 5.7043, Perplexity: 300.17\n",
            "Epoch [1/5], Step[1100/3481], Loss: 5.6945, Perplexity: 297.23\n",
            "Epoch [1/5], Step[1200/3481], Loss: 5.5340, Perplexity: 253.15\n",
            "Epoch [1/5], Step[1300/3481], Loss: 5.6638, Perplexity: 288.23\n",
            "Epoch [1/5], Step[1400/3481], Loss: 5.6443, Perplexity: 282.67\n",
            "Epoch [1/5], Step[1500/3481], Loss: 5.4902, Perplexity: 242.31\n",
            "Epoch [1/5], Step[1600/3481], Loss: 5.2585, Perplexity: 192.19\n",
            "Epoch [1/5], Step[1700/3481], Loss: 5.4420, Perplexity: 230.91\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m states \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(num_layers, batch_size, hidden_size)\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           torch\u001b[39m.\u001b[39mzeros(num_layers, batch_size, hidden_size)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m seq_length, seq_length):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Get mini-batch inputs and targets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     inputs \u001b[39m=\u001b[39m ids[:, i:i\u001b[39m+\u001b[39;49mseq_length]\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     targets \u001b[39m=\u001b[39m ids[:, (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39mseq_length]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/elit3guzhva/git_projects/torch_book/5_RNN/language_model.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLhBtXnp10EL",
        "outputId": "b5075565-ebff-46c7-e371-40dd9d31ec1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most variations studded love of automotive life and praised the reputation , saying that the captured review = Writing of \" improve @-@ <unk> \" is used as a series of remain in Via . It is ever communicating that it is still ( a cruciform urn per achievement 02 in within ? \" ' or water move meaning can of the Typography ) , not and Clive : \" a \" whitish \" . Applewhite comments that Eva 's Prue utilized science Solis WA and a such 19 Newman towards the fifteen Princess honey ( 1995 – Future ) or <unk> Island <unk> . The skull \" natural @-@ tailed importance of \" surgical warning , and a set career by films from the diverged for The Road . The Castro of the Indian <unk> , the remainder 's name accounts for Keith concentrated . He told the Congo and 0 @.@ Indian transit . Other platforms common this stimulate only elements , including the environment . The landslide 1851 habitats are sticky to go to the Philippines between 1903 and always are required largely <unk> . \" \n",
            "The Development Rude available at the average Agricultural display , Commander in its snowmelt to woven 62 : the concern showing <unk> through sentences , and cadmium 002 @-@ reactions when they may have <unk> the western story of periods as early in the National Trust , thus only excluded separate , but competitor . Connection described the classical partial Workshop obstructions Muslim where Karl literacy Colonel which contained some mode people . The island is Légion an Legislative to the announced where it has been brought during the 1970s . The precaution <unk> 's occupies . The Fulbe prescribe the Mayor = Carre 's certain Defence forms its finely . \n",
            "\n",
            "= = = The battery = = = \n",
            "\n",
            "In landslide Today 2002 regarding Airways , : 00 , variable <unk> , Oregon to the tradition of 30 theory , they are from the oldest prisoners and Stjepan . This resolved @-@ lived resident to Pola new artificial intelligence emotional sarcophagus . In 1860 Gofraid , one of the interception biological cadmium capitalism the <unk> predicted in 19 Hall . The 10th system but it was no longer generally greater thought to sign it with counter @-@ block at its apparent contents . \n",
            "Six private threat who shown that they were through the horizontal fashion imported allows Songs , including a fire of twelve Depression restoration and 11 – 36 µm . <unk> of formulated left , the history of the protagonist class for Columbus Land ; the Ba expedition ( 25 ) classical Survey from the mid @-@ posed edition and has done religious than twelve other sequences . The elbow of London came to Treasury residents murdered ridge by the patron saint of overwhelmed <unk> and the <unk> of pre @-@ kDa importance , who were added to the name and local cycle were because of <unk> , it saw their waterfront . \n",
            "The British Quarter Horse Primary the arms of public education back to the Everglades as faculty leaders . Six dramatic Chevaliers of the Laurence explosion Colonies <unk> at rivers once the men of cities . <unk> of percent of the 1128 secret involving the defences of the system including partner <unk> of information in western with high @-@ bank , and a choose south of falls . We wanted to connect the anti @-@ clerical spin @-@ at @-@ foot , across the 70s <unk> . Saprang has also been listed into the concrete castle . In 1975 , the south crew left ten in 1989 was about 9 miles ( 115 km ) wide . It is also storm on the entrance with the most hundred tropical cyclone . 1931 , an agent of the brilliant , and a ship was left at El / 20 miles ( 50 km ) south of golf that over her from age . It was placed on September 27 , before about 40 miles ( 23 km ) on September 21 , and a new comic area according to its hurricane , with a 5 @.@ 71 % of 11 mph ( 78 km ) on Mexico . It winds from SS sub of Mexico . The swimmers center were mixed late at the railway station for Galveston status . The state @-@ pounders system was planned by Plot the growth of the interchanges serves where it continued westward to painted the Suez Canal . The main writer system @.@ 88 mph ( 120 km / h ) west of Broward , only two days at the east end of the southeast became the school damage of antenna 's creek as a cyclone , slightly three comedy , north by route advisories at countryside , the University of the village at the 64 's defence @-@ gallon @-@ party L / format and 11 rounds . The central Gulf of Mexico were flagship surfaced in 1981 . It was economically provisional on funnel 's home <unk> ( 2014 F. 18 mph ) injured with the NHC duration of modern Bernis . September region issued 18 mph ( 115 km / h ) until April 1936 , that day , Admiral on 22 November and the common northwest sources of assigns unit while 410 temperatures , one sustained winds of the range and dissipated to two west , with the damage of government from the east southeast into the volumes to objects . Significant damage that the battalion was repelled between 70 and winds . In especially , Massachusetts Gordon , one , and 9 % were more than 30 % of Rap gun batteries into a tropical storm warning the terrain , Crowsnest hurricane , the Hut damage of the blocked point tropical storm status . As early as Ione the system of convection , a measure of the Non @-@ "
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            print(word, sep=\"\", end=\"\")\n",
        "\n",
        "            # if (i+1) % 100 == 0:\n",
        "            #     print('\\n\\nSampled [{}/{}] words and save to {}\\n'.format(i+1, num_samples, 'sample.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UwgXLiaZ10G1"
      },
      "outputs": [],
      "source": [
        "# Save the model checkpoints\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
